{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76eb8307-1348-462a-8fbb-8d5374d857ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, AutoConfig, AdamW, get_scheduler\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import flask\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1424694a-d957-40bf-849f-2d2ef3cd1009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Defining checkpoint and getting the model and tokenizer\n",
    "checkpoint = \"bert-base-uncased\" # \n",
    "config = AutoConfig.from_pretrained(checkpoint, num_labels=1, problem_type=\"regression\") # Chose regression because the WA values are continuous in the dataset\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0140910e-f863-490f-85ba-357dff09e3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['argument', 'topic', 'set', 'WA', 'MACE-P', 'stance_WA', 'stance_WA_conf'],\n",
       "        num_rows: 30497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing the dataset\n",
    "raw_dataset = load_dataset('csv', data_files = \"arg_quality_rank_30k.csv\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7a29c2-4232-4170-9cb9-64ef10b2a719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['argument', 'topic', 'set', 'WA', 'MACE-P', 'stance_WA', 'stance_WA_conf'],\n",
       "        num_rows: 24397\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['argument', 'topic', 'set', 'WA', 'MACE-P', 'stance_WA', 'stance_WA_conf'],\n",
       "        num_rows: 6100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, this dataset only has train data. So, I will split the data in 80-20, so that I can have train data and evaluate data to finetune\n",
    "from datasets import DatasetDict\n",
    "train_test_split = raw_dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47da7b6a-b267-4aeb-a44e-d59f867948e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ensuring it runs on GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928da7d8-8a10-4223-a474-45cc7c6d5701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the tokenize function\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset['argument'], padding = True, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9521e1ee-3f34-490a-8f8e-1428431c4802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 24397/24397 [00:03<00:00, 6973.64 examples/s]\n",
      "Map: 100%|██████████| 6100/6100 [00:00<00:00, 7792.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'argument': ['people are living healthier longer now and forcing retirement based on arbitrary guidelines like age is a bad idea.', 'factory farming causes cruelty to animals', 'it is reverse discrimination and only benefits certain minorities. asians do not benefit from it.', 'although we hope never to use them, we need to keep our nuclear weapons as a deterrent against others using them therefore we should not abolish them.', 'we should not abandon the use of school uniforms because it helps to create a fair and even playing field between students who are rich and poor alike.'], 'topic': ['We should end mandatory retirement', 'We should ban factory farming', 'We should end affirmative action', 'We should fight for the abolition of nuclear weapons', 'We should abandon the use of school uniform'], 'set': ['train', 'test', 'train', 'train', 'dev'], 'WA': [0.941212704, 1.0, 0.833479962, 0.943428971, 0.864049819], 'MACE-P': [0.886630134, 0.977731841, 0.432486809, 0.873463115, 0.785187507], 'stance_WA': [1, 1, 1, -1, -1], 'stance_WA_conf': [0.892797891, 1.0, 0.901680838, 1.0, 1.0], 'input_ids': [[101, 2111, 2024, 2542, 2740, 3771, 2936, 2085, 1998, 6932, 5075, 2241, 2006, 15275, 11594, 2066, 2287, 2003, 1037, 2919, 2801, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4713, 7876, 5320, 18186, 2000, 4176, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 2003, 7901, 9147, 1998, 2069, 6666, 3056, 14302, 1012, 4004, 2015, 2079, 2025, 5770, 2013, 2009, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2348, 2057, 3246, 2196, 2000, 2224, 2068, 1010, 2057, 2342, 2000, 2562, 2256, 4517, 4255, 2004, 1037, 28283, 22787, 2114, 2500, 2478, 2068, 3568, 2057, 2323, 2025, 11113, 20872, 2232, 2068, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2057, 2323, 2025, 10824, 1996, 2224, 1997, 2082, 11408, 2138, 2009, 7126, 2000, 3443, 1037, 4189, 1998, 2130, 2652, 2492, 2090, 2493, 2040, 2024, 4138, 1998, 3532, 11455, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched = True)\n",
    "print(tokenized_dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf9c5eb-786c-4beb-9c44-9f5161f1c85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "data_collator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d15fe0df-dd97-4596-9265-6df038d4bbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['argument', 'topic', 'set', 'WA', 'MACE-P', 'stance_WA', 'stance_WA_conf', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 24397\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['argument', 'topic', 'set', 'WA', 'MACE-P', 'stance_WA', 'stance_WA_conf', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 6100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87299d22-9626-489a-ab72-7f96977ee631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 24397\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, for the full training, we need to remove the columns that the model does not expect and rename stance to label\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"argument\", \"topic\", \"set\", \"stance_WA\", \"MACE-P\", \"stance_WA_conf\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"WA\", \"labels\") # WA and MACE-P both have continous values, and are both labels. I chose WA, but can choose MACE-P as well\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a11693e9-da69-4a0b-b3b2-ec6a1eff228c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9412, 1.0000, 0.8335,  ..., 1.0000, 0.7650, 0.9060])\n",
      "tensor([0.8102, 0.6280, 0.8768,  ..., 0.4755, 0.7565, 0.7495])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][\"labels\"])\n",
    "print(tokenized_dataset[\"validation\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07064fb5-583f-40db-8c0b-22f94b8bf69f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 59]),\n",
       " 'token_type_ids': torch.Size([8, 59]),\n",
       " 'attention_mask': torch.Size([8, 59])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we define our dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "# To quickly inspect there is no mistake\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "190b3bc5-7651-4356-9923-f960b730cf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Moving the batch to the same device as the model\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05135443-eb01-47be-b66d-c7483dbd99d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sghimire26/.local/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 9150/9150 [05:28<00:00, 28.91it/s]"
     ]
    }
   ],
   "source": [
    "# Now the training loop\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = nn.MSELoss()(outputs.logits.squeeze(), batch[\"labels\"])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16a52835-e1f5-4c1a-88dc-919644c7090c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NLP/To_Download/tokenizer_config.json',\n",
       " 'NLP/To_Download/special_tokens_map.json',\n",
       " 'NLP/To_Download/vocab.txt',\n",
       " 'NLP/To_Download/added_tokens.json',\n",
       " 'NLP/To_Download/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"trained_model\")\n",
    "tokenizer.save_pretrained(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afae1311-6433-4e90-aac2-3440da65f0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A test function\n",
    "def evaluate_argument_quality(argument):\n",
    "    inputs = tokenizer(argument, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        score = outputs.logits.squeeze().cpu().numpy()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef835b77-d952-4275-ba56-3f3d087684e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Score of argument: 0.36732128262519836\n",
      "Quality Score of argument: 0.5132728815078735\n",
      "Quality Score of argument: 0.5154016613960266\n",
      "Quality Score of argument: 0.9191880226135254\n"
     ]
    }
   ],
   "source": [
    "arguments = [\n",
    "    \"Your wife was seen in the market. So, I assumed your wife was not at home.\",\n",
    "    \"Cows have four legs. Donkeys have four legs. So cows are donkeys.\",\n",
    "    \"The sky is blue during the day.\",\n",
    "    \"Water boild at 100 degrees celsius.\"\n",
    "]\n",
    "\n",
    "for arg in arguments:\n",
    "    score = evaluate_argument_quality(arg)\n",
    "    print(f\"Quality Score of argument: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89938c-c766-4ba7-8fc4-6dc05774fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 via SLURM GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
